An error occurred while executing the following cell:
------------------
for type in algo_list:
    if type == "bald":
        print("******************************************BALD Implementation******************************************")
    else:
        print(
            "******************************************BatchBALD Implementation******************************************"
        )

    for acquisition_batch_size in batch_list:  # Batch size per iteration
        print(
            "******************************************Batch Size: "
            + str(acquisition_batch_size)
            + "******************************************"
        )

        seed_value = 0

        torch.manual_seed(seed_value)
        torch.cuda.manual_seed(seed_value)
        torch.backends.cudnn.deterministic = True
        torch.cuda.manual_seed_all(seed_value)
        random.seed(seed_value)
        np.random.seed(seed_value)
        os.environ["PYTHONHASHSEED"] = str(seed_value)

        num_initial_samples = 20  # Number of initial samples required
        num_classes = 10  # Total classes in MNIST dataset

        train_dataset, test_dataset = repeated_mnist.create_repeated_MNIST_dataset(num_repetitions=1, add_noise=False)

        # Generates 20 samples (2 from each class) and returns their indices
        initial_samples = active_learning.get_balanced_sample_indices(
            repeated_mnist.get_targets(train_dataset),
            num_classes=num_classes,
            n_per_digit=num_initial_samples / num_classes,
        )

        test_accs = []
        test_loss = []
        added_indices = []

        active_learning_data = active_learning.ActiveLearningData(
            train_dataset
        )  # Splits the dataset into training dataset and pool dataset

        active_learning_data.acquire(
            initial_samples
        )  # Seperates the initial indices from the pool and fixes it as initial train dataset
        active_learning_data.extract_dataset_from_pool(
            40000
        )  # Extracts 40000 samples from pool and makes it as validation dataset

        train_loader = torch.utils.data.DataLoader(
            active_learning_data.training_dataset,
            sampler=active_learning.RandomFixedLengthSampler(
                active_learning_data.training_dataset, training_iterations
            ),
            batch_size=batch_size,
            **kwargs,
        )

        pool_loader = torch.utils.data.DataLoader(
            active_learning_data.pool_dataset,
            batch_size=scoring_batch_size,
            shuffle=False,
            **kwargs,
        )

        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, **kwargs)

        pbar = tqdm(
            initial=len(active_learning_data.training_dataset),
            total=max_training_samples,
            desc="Training Set Size",
        )

        while True:
            model = BayesianCNN(num_classes).to(device=device)  # initialise model
            optimizer = torch.optim.Adam(model.parameters())

            model.train()

            # Train
            for data, target in tqdm(train_loader, desc="Training", leave=False):
                data = data.to(device=device)
                target = target.to(device=device)

                optimizer.zero_grad()

                prediction = model(data, 1).squeeze(1)
                loss = F.nll_loss(prediction, target)

                loss.backward()
                optimizer.step()

            # Test
            loss = 0
            correct = 0
            with torch.no_grad():
                for data, target in tqdm(test_loader, desc="Testing", leave=False):
                    data = data.to(device=device)
                    target = target.to(device=device)

                    prediction = torch.logsumexp(model(data, num_test_inference_samples), dim=1) - math.log(
                        num_test_inference_samples
                    )
                    loss += F.nll_loss(prediction, target, reduction="sum")

                    prediction = prediction.max(1)[1]
                    correct += prediction.eq(target.view_as(prediction)).sum().item()

            loss /= len(test_loader.dataset)
            test_loss.append(loss)

            percentage_correct = 100.0 * correct / len(test_loader.dataset)
            test_accs.append(percentage_correct)

            print("Test set: Average loss: {:.4f}, Accuracy: ({:.2f}%)".format(loss, percentage_correct))

            if len(active_learning_data.training_dataset) >= max_training_samples:
                break

            # Acquire pool predictions
            N = len(active_learning_data.pool_dataset)
            logits_N_K_C = torch.empty(
                (N, num_inference_samples, num_classes),
                dtype=torch.double,
                pin_memory=use_cuda,
            )

            with torch.no_grad():
                model.eval()

                for i, (data, _) in enumerate(tqdm(pool_loader, desc="Evaluating Acquisition Set", leave=False)):
                    data = data.to(device=device)

                    lower = i * pool_loader.batch_size
                    upper = min(lower + pool_loader.batch_size, N)
                    logits_N_K_C[lower:upper].copy_(model(data, num_inference_samples).double(), non_blocking=True)

            with torch.no_grad():
                if type == "batchbald":
                    candidate_batch = batchbald.get_batchbald_batch(
                        logits_N_K_C,
                        acquisition_batch_size,
                        num_samples,
                        dtype=torch.double,
                        device=device,  # Returns the indices and scores(Mutual Information) for the batch selected by Batchbald/BALD Strategy.
                    )

                elif type == "bald":
                    candidate_batch = batchbald.get_bald_batch(
                        logits_N_K_C,
                        acquisition_batch_size,
                        dtype=torch.double,
                        device=device,
                    )

            targets = repeated_mnist.get_targets(active_learning_data.pool_dataset)  # Returns the target labels
            dataset_indices = active_learning_data.get_dataset_indices(
                candidate_batch.indices
            )  # Returns indices for candidate batch

            print("Dataset indices: ", dataset_indices)
            # print("Scores: ", candidate_batch.scores)
            print("Labels: ", targets[candidate_batch.indices])

            active_learning_data.acquire(candidate_batch.indices)  # add the new indices to training dataset
            added_indices.append(dataset_indices)
            pbar.update(len(dataset_indices))
        final_test_accs.append(test_accs)
        final_indices.append(added_indices)
------------------

---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
/tmp/ipykernel_4389/1385880512.py in <module>
    154                         acquisition_batch_size,
    155                         dtype=torch.double,
--> 156                         device=device,
    157                     )
    158 

~/miniconda3/envs/py37/lib/python3.7/site-packages/batchbald_redux/batchbald.py in get_bald_batch(log_probs_N_K_C, batch_size, dtype, device)
    116     candidate_scores = []
    117 
--> 118     scores_N = -compute_conditional_entropy(log_probs_N_K_C)
    119     scores_N += compute_entropy(log_probs_N_K_C)
    120 

~/miniconda3/envs/py37/lib/python3.7/site-packages/batchbald_redux/batchbald.py in compute_conditional_entropy(log_probs_N_K_C)
     24     pbar = tqdm(total=N, desc="Conditional Entropy", leave=False)
     25 
---> 26     @toma.execute.chunked(log_probs_N_K_C, 1024)
     27     def compute(log_probs_n_K_C, start: int, end: int):
     28         nats_n_K_C = log_probs_n_K_C * torch.exp(log_probs_n_K_C)

~/miniconda3/envs/py37/lib/python3.7/site-packages/toma/__init__.py in chunked(tensor, initial_step, dimension, cache_type, context)
    196             context=None,
    197         ):
--> 198             context = context or tst.get_simple_traceback(1)
    199 
    200             def execute_chunked(func):

~/miniconda3/envs/py37/lib/python3.7/site-packages/toma/stacktrace.py in get_simple_traceback(ignore_top)
     17     """Get a simple trackback that can be hashed and won't create reference
     18     cyles."""
---> 19     stack = inspect.stack(context=1)[ignore_top + 1 : -__watermark - 1]
     20     simple_traceback = tuple(
     21         (fi.filename, fi.lineno, fi.function, _constant_code_context(fi.code_context), fi.index) for fi in stack

~/miniconda3/envs/py37/lib/python3.7/inspect.py in stack(context)
   1511 def stack(context=1):
   1512     """Return a list of records for the stack above the caller's frame."""
-> 1513     return getouterframes(sys._getframe(1), context)
   1514 
   1515 def trace(context=1):

~/miniconda3/envs/py37/lib/python3.7/inspect.py in getouterframes(frame, context)
   1488     framelist = []
   1489     while frame:
-> 1490         frameinfo = (frame,) + getframeinfo(frame, context)
   1491         framelist.append(FrameInfo(*frameinfo))
   1492         frame = frame.f_back

~/miniconda3/envs/py37/lib/python3.7/inspect.py in getframeinfo(frame, context)
   1462         start = lineno - 1 - context//2
   1463         try:
-> 1464             lines, lnum = findsource(frame)
   1465         except OSError:
   1466             lines = index = None

~/miniconda3/envs/py37/lib/python3.7/inspect.py in findsource(object)
    826         pat = re.compile(r'^(\s*def\s)|(\s*async\s+def\s)|(.*(?<!\w)lambda(:|\s))|^(\s*@)')
    827         while lnum > 0:
--> 828             if pat.match(lines[lnum]): break
    829             lnum = lnum - 1
    830         return lines, lnum

IndexError: list index out of range
IndexError: list index out of range
